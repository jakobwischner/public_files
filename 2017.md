# Big Data Analytics 2016/17
final exam - 75 points

## Aufgabe 1 - Variance and Covariance - 7 points

### a)
Define the expectation value of a random variable X

E[X] =

### b)
Define the variance of a random variable X

Var(X) =

### c)
Define the covariance of two random variables X and Y

Cov(X,Y) = 

### d)
Show that if two random variables X anc Y are independent, then their covariance is zero. You can use E[XY] = E[X] * E[Y] for independent random variables X and Y.

## Aufgabe 2 - Statistical Hypothesis Testing - 9 points

You have a coin and want to test whether the coin is fair or biased, using a statistical hpothesis test. You throw the coin n times and note down how many times it showed heads and tails, respectively. Define your own statistical test for this purpose.

### a)
Specify the null hypothesis H_0 and the alternative hypothesis H_1 for this test.

H_0:

H_1: 

### b)
Provide the formula for a test statistic t that is suitable for this purpose (Hint: think of expected and observed counts).

t =

### c)
Briefly (2-3 sentences) explain how you can use the test statistic value obtained from a sampe experiment to assess the coin. Include the concepts of __critical value__ and __significance level__ in your explanation.

## Aufgabe 3 - Clustering - 4 points
Suppose the clusters visualiyed on the right hand side were found by a clustering algoritm. Is this k-Means (k=3, Euclidian Distance) or DBSCAN (e=1.5, MinPts=3, Euclidian Distance)?

Give reasons for your answers (1-2 sentences each).

Pointset from diagram - Points (x,y)

|Cluster 1|Cluster 2|
|:---:|:---:|
1, 4| 4, 3
2, 4| 5, 3
2, 5| 5, 2
3, 5| 5, 1
3, 6| 6, 3
4, 5| 6, 2
4, 4| 
 
### a)
k-means

### b)
DBSCAN

## Aufgabe 4 - Determining e-values using k-NN plots - 5 points
Given the following 2-dimensional data set:

Diagramm:

|X|Y|
|:---:|:---:|
6|6
5|7
7|&
6|8
8|0
10|0
1|5
0|6
2|6
1|7

Use the heuristic from the lecture to determine input parameters MinPts and e for DBSCAN (sing Manhattan Distance). You may use the image on the right.

## Aufgabe 5 - Silhuette coefficient - 6 points
Calculate the silhuette coefficient for a clustering C_1={P_1, P_2} and C_2={P_3, P_4} of the 2-dimensional data set {P_1=(3, 4), P_2=(2, 3), P_3=(3, 3), P_4=(4, 2)} using the Manhattan distance. According to the interpretation of silhuette coefficients from the lecture, how is the overall structure of the clustering rated?

[Diagramm]

## Aufgabe 6 - NN-Classification - 4 points
Consider the following data set. These data points belong to two classes:

| black | grey |
|:---:|:---:|
(0, 3)|(1, 2)
(0, 5)|(2, 0)
(1, 6)|(2, 5)
(3, 0)|(2, 7)
(3, 5)|(3, 4)

original: unsorted set, but with a diagram on the side

Use the three nearest neighbour classifier to classify the data points Q_1=(0, 2), Q_2=(4, 5)

Use Manhattan distance to identify the 3-NN and the majority voting criteria to determine the class (make sure to include the decision sets in your solution). You may use the visualization above to determine the 3-NN.

## Aufgabe 7 - Decision Trees and Minimal cost complexity pruning - 11 points
Given the following training set:

|no.|outlook|temperature|humidity|class|
|:---:|:---:|:---------:|:------:|:---:|
1|sunny     |hot        |high    |NO
2|sunny     |hot        |high    |NO
3|overcast  |hot        |high    |YES
4|rain      |mild       |high    |NO
5|rain      |cool       |normal  |YES
6|rain      |cool       |normal  |YES
7|overcast  |cool       |normal  |YES
8|sunny     |mild       |high    |NO

### a)
Motivate briefly why a desicion tree is a suitable classifier for this data set?

### b)
Given the decision tree below, perform the first pruning step according to 'minimal cost complexity pruning'. Explain all calculations and all used formulas!

[Tree]

1. outlook sunny
    1. humidity high
        1. NO
    1. humidity normal
        1. YES
1. outlook overcast
    1. YES
1. outlook rain
    1. temperature high
        1. YES
    1. temperature mild
        1. NO
    1. temperature cool
        1. YES

## Aufgabe 8 - Naiive Bayes Classification - 9 points
Suppose your want to use the naiive Bayes Classifier approach to distinguish two classes C_1 and C_2. The input data contains two categorical attributes X and Y. The X attribute consists of three possible values i, k and l while the Y attribute has a domain of four different values i, k, l and m.

### a)
Write down the decision rule for the naiive Bayesian classifier.

### b)
Use the following dataset (class,X,Y) to train your naiive Bayesian classifier:

(C_1, i, i)

(C_1, k, k)

(C_1, i, k)

(C_1, k, l)

(C_2, i, l)

(C_2, k, m)

(C_2, k, k)

(C_2, l, m)

(C_2, l, l)

Illustrate the conditional probability distribuions for each class by specifiying the corresponding histograms.

### c)
Use the classifier from above to predict a class label for the following two data points:
#### a. (?,i,k)
#### b. (?,k,m)

## Aufgabe 9 - Properties of Classifiers - 6 points
Name three problems than can occur when training classifiers, explain briefly how the occur and give one example each.

## Aufgabe 10 - Itemset Mining - 14 points
The following table shows the contents of a transaction data base. The left column contains the itemset, and the right column shows the frequency of the itemset in the data base.
### a)
Construct an FP-Tree for the given data. Use a minimum support of 2.

AE: 1

CEG: 3

BEH: 1

ABE: 2

BE: 3

CEFG: 1

EG: 2

BDE: 1

ACEG: 2

### b)
Now extract all frequent itemsets from the FP-Tree. Again, use a minimum support of 2. For each step of the extraction algorithm write down the conditional pattern base, the conditional FP-Tree, and the itemsets which are added to the result set.
